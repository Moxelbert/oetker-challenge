Dear reader,

thanks for taking the time to review my code. Here is how to run it (you need Docker):

This is how you run the code
1.	Open Terminal, go to the directory of the code and run “docker build -t oetker-challenge .”
2.	Run "docker images" to get a list of images (there should be one with the name oetker-challenge)
3.	Run "docker run -i oetker-challenge:latest" (if you gave the image a different name in step 1, use this name instead)
4.	Open a second terminal and run “docker ps”  to see a list of containers
5.	“Docker exec -it <container id> bash” starts shell in container
6.	(Optional) run "python3 main.py" => optional because the data is already loaded to a database
7.	In the container go to /API and run "python3 main_api.py"

how to push an image to Docker hub
1. "docker login --username username"
2. prompts for password 
3. "docker tag my-image username/my-repo"
4. "docker push username/my-repo"

Airflow Version 1
1. Open a Terminal and run "docker pull puckel/docker-airflow"
2. Next step is to run image with "docker run -d -p 8080:8080 puckel/docker-airflow webserver" ==> This will run airflow and you can access webui at localhost:8080
3. To copy dags use this command "docker cp sample_dag.py containerName:/usr/local/airflow/dags"
4. To access airflow utility you need to access the bash shell of container by using: 
	"docker exec -it containerName bash" 
5. Once you inside bash shell you can run command line utilities ex **airflow list_dags**

==> General hint: it is possible that after copying a dag into Airflow, there is a message like "Broken DAG: [/usr/local/airflow/dags] No module named 'docker'"
		  if that is the case, you either 
			- need to configure the puckel/docker-airflow image by adding the installation command for the module in question
			- enter the container and install the module there via pip3 install


Airflow Version 2 (recommended)
(Check here for questions https://www.youtube.com/watch?v=aTaytcxy2Ck)
1. Get the docker-compose.yaml from https://github.com/apache/airflow/blob/main/docs/apache-airflow/start/docker-compose.yaml (set AIRFLOW__CORE__LOAD_EXAMPLES = 'false')
2. create a directory the yaml. In the same directory, create 3 folders "dags", "logs", "plugin"
3. also create a .env file with the values 
	AIRFLOW_UID=50000
	AIRFLOW_GID=0 ==> make sure to save it as a textfile
4. in the directory, run "docker-compose up airflow-init"
5. Afterwards, run "docker-compose up"
6. Check localhost 8080 for the UI. PW and USER are "airflow"
7. To copy dags use this command "docker cp test_dag.py cb7c46c420a1:/opt/airflow/dags" (can be different ==> check in docker-compose.yaml


